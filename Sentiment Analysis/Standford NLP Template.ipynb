{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StandfordCoreNLP\n",
    "#### Use this API to get sentiment score (via PyCorenlp wrapper)\n",
    "\n",
    "See https://stackoverflow.com/questions/32879532/stanford-nlp-for-python\n",
    "\n",
    "Since StandfordCoreNLP is written in Java, we need to use pycorenlp (a python wrapper for StandfordCoreNLP) to connect to Java and use it here. To run the following codes, you need to install the following:\n",
    "\n",
    "1. Install latest StandfordCoreNLP version (a zip file) from http://nlp.stanford.edu/software/stanford-corenlp-latest.zip \n",
    "    - Note: In MacOS, you can directly download by using wget or curl\n",
    "\n",
    "2. Unzip the downloaded file, put it into a directory, say named `standford-corenlp-4.0.0`.\n",
    "\n",
    "3. Start the server using terminal\n",
    "    - Open the terminal, use `cd` to go to the directory you just created, type `cd Users\\xxx\\stanstandford-corenlp-4.0.0`\n",
    "    - Then type in `java -mx5g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -timeout 10000` \n",
    "    - Now you should see something like `[main] INFO CoreNLP - StanfordCoreNLPServer listening at /0:0:0:0:0:0:0:0:9000`, means that your server is started, waiting for data from port in 9000\n",
    "   \n",
    "4. Then download the `pycorenlp` package using `pip install pycorenlp` \n",
    "5. Now you should be good to run the following sentiment anaylsis code using StandfordCoreNLP (note the port below is also using 9000 as you just set. If port name not consistent, your code would fail)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "import string\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "import pprint\n",
    "\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models import Word2Vec, CoherenceModel\n",
    "\n",
    "from Data_Processor import clean, Data_Processor\n",
    "\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([9513, 9768, 9885, 9534, 9371, 9661, 10183, 9527, 9292, 9290, 9059, 9399, 9927, 9314, 9829, 9700, 9283, 9411, 9914, 9075, 8763, 9295, 8949, 9234, 8468, 8648, 9049, 8523, 9401, 8371, 7937, 8180, 8683, 9569, 8916, 9644], 332565)\n",
      "([2129, 2492, 2770, 2498, 2423, 2481, 2387, 2563, 2221, 2036, 2126, 2878, 2602, 2066, 2146, 2745, 2096, 1885, 2462, 1846, 2046, 2658, 2298, 2876, 2647, 1632, 2031, 2538, 2128, 2122, 2057, 2389, 2242, 2557, 2490, 2711], 84274)\n"
     ]
    }
   ],
   "source": [
    "key = 'Morgan_Stanley'\n",
    "DP=Data_Processor(start_month='2017-06',end_month='2020-05',\n",
    "                  template=[\"../../Data/\"+key])\n",
    "                  #\"../../Data/UBS\",\"../../Data/Goldman_Sachs\"\n",
    "\n",
    "DP.readdata()\n",
    "print(DP.datanums())\n",
    "DP.specifylang()\n",
    "DP.removenoise() \n",
    "DP.clean()\n",
    "\n",
    "#after removing noises (delete promotional/advertisement data)\n",
    "print(DP.datanums())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## do not tokenize for sentiment data\n",
    "#DP.tokenizetext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2017-06', '2017-07', '2017-08', '2017-09', '2017-10', '2017-11', '2017-12', '2018-01', '2018-02', '2018-03', '2018-04', '2018-05', '2018-06', '2018-07', '2018-08', '2018-09', '2018-10', '2018-11', '2018-12', '2019-01', '2019-02', '2019-03', '2019-04', '2019-05', '2019-06', '2019-07', '2019-08', '2019-09', '2019-10', '2019-11', '2019-12', '2020-01', '2020-02', '2020-03', '2020-04', '2020-05']\n"
     ]
    }
   ],
   "source": [
    "months = list(DP.textdata().keys())\n",
    "print(months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT = 100000 #standfordnlp can only process 100000 characters a time, need divide them into serveral parts\n",
    "nlp = StanfordCoreNLP('http://localhost:9000') #the port here should be the same as above (openned server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(data, month):\n",
    "    all_sentences = \" . \".join(data)\n",
    "    n_divid = len(all_sentences)//LIMIT*3\n",
    "    len_divid = len(data)//n_divid\n",
    "    print(f\"Processing {month} - length {len(data)}, {n_divid} parts\")\n",
    "    \n",
    "    #nlp.annotate will return a dictionary (key is 'sentences')\n",
    "    #here nlp.annotate process each sentence in the paragraph above using the annotators specified below\n",
    "    #res = {}\n",
    "    all_sentence = []\n",
    "    sentiment = []  #save sentiment (\"Positive\", \"Netrual\", \"Negative\")\n",
    "    score = []  #save sentiment score\n",
    "\n",
    "    for i in range(n_divid+1): #one extra divid for remaining part\n",
    "        #print(i*len_divid, (i+1)*len_divid)\n",
    "        if i==n_divid and i*len_divid>=len(data):\n",
    "            break\n",
    "        \n",
    "        sentence = \" . \".join(data[i*len_divid:min((i+1)*len_divid,len(data))]) #take min to include the last remaining part\n",
    "        \n",
    "        res = nlp.annotate(sentence,\n",
    "                           properties={\n",
    "                           'annotators': 'tokenize,ssplit,pos,parse,sentiment',\n",
    "                           'outputFormat': 'json',\n",
    "                           'timeout': 100000,\n",
    "                           })\n",
    "        if isinstance(res,str):\n",
    "            raise Exception(\"Sentence is too long to parse\")\n",
    "\n",
    "        #add sentiment result\n",
    "        for s in res['sentences']:\n",
    "            #all_sentence.append(\" \".join([t[\"word\"] for t in s[\"tokens\"]]))\n",
    "            score.append(s[\"sentimentValue\"])\n",
    "            sentiment.append(s[\"sentiment\"])\n",
    "\n",
    "    \n",
    "    return score, sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save to json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2017-06 - length 2129, 3 parts\n",
      "Processing 2017-07 - length 2492, 6 parts\n"
     ]
    }
   ],
   "source": [
    "sentiment = {}\n",
    "for i,m in enumerate(months):\n",
    "    score, sentiment = sentiment_analysis(DP.textdata()[m], m)  #get two lists of data that month\n",
    "    for j in range(len(DP.data[i])):\n",
    "        DP.data[i][j]['sentiment_score'] = score[j]\n",
    "        DP.data[i][j]['sentiment'] = sentiment[j]\n",
    "    \n",
    "    with open(key+m+'.json',\"w\") as file:\n",
    "        json.dump(DP.data[i], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
